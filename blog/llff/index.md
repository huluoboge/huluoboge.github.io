---
title: "Local Light Field Fusion Practical View Synthesis with Prescriptive Sampling Guidelines"
date: 2025-10-22T23:07:00+08:00
tags:
  [
    "Neural Radiance Fields",
    "NeRF",
    "Renderer",
    "Computer Vision",
    "Computer Graphics",
  ]
excerpt: "翻译：Local Light Field Fusion Practical View Synthesis with Prescriptive Sampling Guidelines"
draft: false
---

https://bmild.github.io/llff/

# 局部光场融合：具有规定采样指南的实用视图合成

**arXiv:1905.00889v1 [cs.CV] 2019年5月2日**

**作者：**
- BEN MILDENHALL∗，加州大学伯克利分校
- PRATUL P. SRINIVASAN∗，加州大学伯克利分校  
- RODRIGO ORTIZ-CAYON，Fyusion公司
- NIMA KHADEMI KALANTARI，德克萨斯农工大学
- RAVI RAMAMOORTHI，加州大学圣地亚哥分校
- REN NG，加州大学伯克利分校
- ABHISHEK KAR，Fyusion公司

**∗ 表示同等贡献**

## 摘要

**快速简便的手持采集指南：**
最近物体在视图间最多移动D像素

**通过分层场景表示**
将采样视图提升为局部光场

**混合相邻局部光场**
以渲染新视图

**图1.** 我们提出了一种简单可靠的方法，用于从手持相机在不规则网格模式中捕获的一组输入图像进行视图合成。我们从理论和实验上证明，我们的方法具有规定采样率，比奈奎斯特采样需要少4000倍的输入视图，即可实现自然场景的高保真视图合成。具体来说，我们表明这个速率可以解释为对捕获视图间相机最近物体的像素空间视差的要求（第3节）。捕获后，我们将所有采样视图扩展为可以渲染高质量局部光场的分层表示。然后我们混合相邻局部光场的渲染结果，以合成密集的新视图路径（第4节）。我们的渲染包含简单快速的计算（单应性变换和alpha合成），可以实时生成新视图。

我们提出了一种实用且鲁棒的深度学习解决方案，用于捕获和渲染复杂真实世界场景的新视图，用于虚拟探索。先前的方法要么需要难以实现的密集视图采样，要么几乎没有提供关于用户应如何采样场景视图以可靠渲染高质量新视图的指导。相反，我们提出了一种从不规则网格采样视图进行视图合成的算法，该算法首先通过多平面图像（MPI）场景表示将每个采样视图扩展为局部光场，然后通过混合相邻局部光场来渲染新视图。我们扩展了传统的光场采样理论，推导出一个边界，精确指定了用户在使用我们的算法时应如何密集采样给定场景的视图。在实践中，我们应用这个边界来捕获和渲染真实世界场景的视图，这些视图实现了奈奎斯特速率视图采样的感知质量，同时使用多达4000倍更少的视图。

我们通过增强现实智能手机应用程序展示了我们方法的实用性，该应用程序指导用户捕获场景的输入图像，并在桌面和移动平台上实现实时虚拟探索的查看器。

**CCS概念：** • 计算方法 → 基于图像的渲染。

**附加关键词和短语：** 视图合成、光场采样、光场、基于图像的渲染、深度学习

**ACM参考格式：**
Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. ACM Trans. Graph. 38, 4, Article 29 (2019年7月), 14页. https://doi.org/10.1145/3306346.3322980

---

# 1 引言

最具吸引力的虚拟体验能够完全将观众沉浸在一个场景中，这种体验的一个标志性特征是从近距离交互距离观察场景的能力。这在合成渲染的场景中目前是可能的，但对于真实世界场景的虚拟体验来说，实现这种亲密程度一直非常困难。

理想情况下，我们可以简单地采样场景的光场，并插值相关的捕获图像来渲染新视图。这种光场采样策略特别有吸引力，因为它们将基于图像的渲染（IBR）问题置于信号处理框架中，我们可以直接推理任何给定场景所需的采样视图密度和模式。然而，对于具有交互距离内容的场景，奈奎斯特速率视图采样是不可行的，因为所需的视图采样率与最近场景深度的倒数成线性增长。例如，对于一个主体深度为0.5米的场景，使用64°视场的手机相机捕获并以1百万像素分辨率渲染，所需的采样率是难以实现的每平方米250万张图像。由于捕获所有所需图像不可行，IBR社区已转向利用几何估计来预测缺失视图的视图合成算法。

最先进的算法将视图合成问题表述为从非结构化集合或任意稀疏网格的输入相机视图预测新视图。虽然这种问题陈述的通用性很吸引人，但放弃光场采样框架牺牲了严格推理这些方法的视图采样要求并预测其性能将如何受输入视图采样模式影响的关键能力。当面对新场景时，这些方法的用户只能通过试错来确定一组采样视图是否会产生可接受的虚拟体验结果。

相反，我们提出了一种基于光场采样框架的视图合成方法，可以精确规定用户必须如何密集捕获给定场景以获得可靠的渲染性能。我们的方法在概念上很简单，包含两个主要阶段。我们首先使用深度网络将每个源视图提升为可以渲染有限范围视图的场景分层表示，推进了多平面图像（MPI）表示的最新工作[Zhou et al. 2018]。然后我们通过混合相邻分层表示的渲染结果来合成新视图。

我们的理论分析表明，我们的方法所需的输入视图数量随着我们为每个分层场景表示预测的平面数量呈二次方减少，直到相机视场设定的限制。我们通过实验验证了我们的分析，并在实践中应用它来渲染具有与奈奎斯特视图采样相同感知质量的新视图，同时使用多达64² ≈ 4000倍更少的图像。

虽然不可能在完全通用性下打破奈奎斯特限制，但我们表明通过专门针对自然场景子集，可以在大大减少视图采样的情况下实现奈奎斯特级别的性能。这种能力主要归功于我们的深度学习管道，该管道在自然场景的渲染上进行训练，以估计产生局部一致光场的高质量分层场景表示。

总之，我们的主要贡献是：
(1) 光场采样理论的扩展，直接指定用户应如何采样输入图像以使用我们的方法进行可靠的高质量视图合成。
(2) 用于捕获和渲染复杂真实世界场景进行虚拟探索的实用且鲁棒的解决方案。
(3) 证明使用局部分层场景表示的精心设计的深度学习管道实现了最先进的视图合成结果。

我们广泛验证了我们推导的规定视图采样要求，并证明我们的算法在一系列亚奈奎斯特视图采样率下在数量上优于传统光场重建方法以及最先进的视图插值算法。我们通过开发一个增强现实应用程序来突出我们方法的实用性，该应用程序实现了我们推导的采样指南，帮助用户捕获输入图像，这些图像使用我们的算法产生可靠的高质量渲染。此外，我们开发了移动和桌面查看器应用程序，可以实时从我们预测的分层表示渲染新视图。最后，我们定性地证明我们的算法在一组多样化的复杂真实世界场景中可靠地产生了最先进的结果。

---

# 2 相关工作

基于图像的渲染（IBR）是从采样视图渲染物体和场景新视图的基本计算机图形学问题。我们发现按照它们使用显式场景几何的程度来分类IBR算法是有用的，正如Shum和Kang [2000]所做的那样。

## 2.1 光场采样与重建

光场渲染[Levoy and Hanrahan 1996]避免任何几何推理，简单地采样规则网格上的图像，以便新视图可以作为采样光场的切片进行渲染。Lumigraph渲染[Gortler et al. 1996]表明使用近似场景几何可以改善由于欠采样或不规则采样视图引起的伪影。

光场采样框架[Chai et al. 2000]使用信号处理技术分析光场渲染，并显示光场的奈奎斯特视图采样率取决于最小和最大场景深度。此外，他们讨论了如何通过更多场景几何知识来降低奈奎斯特视图采样率。Zhang和Chen [2003]扩展了这一分析，展示了非朗伯和遮挡效应如何增加光场的频谱支持，并提出了更一般的视图采样晶格模式。

基于光场采样的渲染算法享有规定采样的显著优势；给定一个新场景，很容易计算所需的视图采样密度以实现高质量渲染。许多现代光场采集系统都是基于这些原理设计的，包括大规模相机系统[Overbeck et al. 2018; Wilburn et al. 2005]和手机应用程序[Davis et al. 2012]。

我们认为规定采样对于实用和有用的IBR算法是必要的，并且我们扩展了先前的光场采样理论，以显示我们基于深度学习的视图合成策略可以显著减少传统光场渲染的密集采样要求。我们的新视图合成管道也可以用于未来的光场采集硬件系统，以减少所需相机的数量。

## 2.2 基于几何的视图合成

许多IBR算法尝试利用显式场景几何从任意非结构化输入视图集合合成新视图。这些方法可以有意义地分类为使用全局或局部几何。

使用全局几何的技术通常从一组非结构化输入图像计算单个全局网格。简单地对这个全局网格进行纹理映射对于受限情况可能是有效的，例如全景观看，其中主要是旋转而很少有平移的观察者移动[Hedman et al. 2017; Hedman and Kopf 2018]，但这种策略只能模拟朗伯材料。表面光场[Wood et al. 2000]能够渲染令人信服的视图相关效果，但它们需要来自密集范围扫描的准确几何和数百张捕获图像来采样物体表面点的出射辐射度。

许多自由视点IBR算法基于局部纹理映射全局网格的策略。有影响力的视图相关纹理映射算法[Debevec et al. 1996]提出了一种通过混合使用全局网格重新投影的附近捕获视图来渲染新视图的方法。关于非结构化Lumigraph渲染的工作[Buehler et al. 2001]专注于计算重新投影图像的每像素混合权重，并提出了一种满足高质量渲染关键属性的启发式算法。不幸的是，很难估计几何边界与图像边缘良好对齐的高质量网格，基于全局几何的IBR算法通常遭受显著的伪影。最先进的算法[Hedman et al. 2018, 2016]试图通过涉及全局网格和局部深度图估计的复杂管道来弥补这一缺点。然而，很难精确定义鲁棒网格估计的视图采样要求，并且网格估计过程通常需要多个小时，使得这种策略对于休闲内容捕获场景不实用。

使用局部几何的IBR算法[Chaurasia et al. 2013; Chen and Williams 1993; Kopf et al. 2013; McMillan and Bishop 1995; Ortiz-Cayon et al. 2015]避免了困难和昂贵的全局网格估计。相反，它们通常为每个输入图像计算详细的局部几何，并通过重新投影和混合附近输入图像来渲染新视图。这种策略也已通过使用第二深度层扩展到模拟非朗伯反射[Sinha et al. 2012]。最先进的Soft3D算法[Penner and Zhang 2017]在重新投影的局部分层表示之间混合以渲染新视图，这在概念上类似于我们的策略。然而，Soft3D通过在大的视图邻域上聚合深度不确定性的启发式度量来计算每个局部分层表示。我们相反训练一个端到端的深度学习管道，通过从更小的视图邻域预测我们的每个局部分层表示来优化新视图质量。此外，我们直接将我们的算法置于光场采样框架内，并且我们的分析也直接适用于Soft3D算法。我们证明，我们深度学习预测的局部场景表示的高质量使我们能够合成优越的渲染，而不需要像Soft3D那样在大的视图邻域上聚合几何估计。这对于渲染非朗伯效应特别有利，因为镜面反射的表观深度通常随观察视点变化，因此在大的视点邻域上平滑估计的几何会阻止这些效应的准确渲染。

其他IBR算法[Anderson et al. 2016]尝试通过使用更一般的2D光流而不是1D深度来插值视图，从而对不正确的相机姿态或场景运动更加鲁棒。局部像素偏移也编码在相位信息中，算法已利用这一点从微基线立体对外推视图[Didyk et al. 2013; Kellnhofer et al. 2017; Zhang et al. 2015]，而无需显式流计算。然而，这些方法需要非常接近的输入视图，不适合大基线视图插值。

## 2.3 用于视图合成的深度学习

其他最近的方法已经训练端到端的深度学习管道用于视图合成。这包括最近的角超分辨率方法[Wu et al. 2017; Yeung et al. 2018]，它们在光场相机的孔径内插值密集视图，但由于不建模场景几何，无法处理更稀疏的输入视图采样。DeepStereo算法[Flynn et al. 2016]、基于深度学习的光场相机视图插值[Kalantari et al. 2016]和单视图局部光场合成[Srinivasan et al. 2017]各自使用深度网络为每个新视图单独预测深度。然而，为每个视图单独预测局部几何会导致在平滑变化视点之间的不一致渲染。

最后，Zhou等人[2018]引入了一个深度学习管道，从窄基线立体对预测MPI，用于立体放大任务。与先前的深度学习视图合成策略相反，这种方法通过使用相同的预测场景表示来渲染所有新视图来强制一致性。我们采用MPI作为我们的局部光场表示，并引入特定的技术改进，以从许多输入视图实现更大基线的视图插值，与使用单个MPI从立体对进行局部视图外推形成对比。我们预测多个MPI，每个输入视图一个，并通过混合过程端到端训练我们的系统，以优化产生的MPI协同用于渲染输出视图。我们提出了一个3D卷积神经网络（CNN）架构，根据输入视图采样率动态调整深度平面数量，而不是具有固定输出平面数量的2D CNN。此外，我们表明最先进的性能只需要一个易于生成的合成数据集和一个小的真实微调数据集，而不是大的真实数据集。这使我们能够生成在2D不规则网格上捕获的训练数据，类似于手持视图采样模式，而Zhou等人[2018]中的YouTube数据集仅限于1D相机路径。

---

# 3 理论采样分析

我们方法的整体策略是使用深度学习管道将每个采样视图提升为具有D个深度层的分层场景表示，并通过混合相邻场景表示的渲染结果来渲染新视图。在本节中，我们展示了我们的深度网络预测的完整场景表示集合可以解释为一种特定形式的光场采样。我们扩展了先前的光场采样工作，以显示我们的策略理论上可以将所需采样视图数量减少D²倍，与传统奈奎斯特视图采样所需的数量相比。第6.1节通过实验表明，我们能够利用这个边界将所需视图数量减少多达64² ≈ 4000倍。

在以下分析中，为了符号清晰，我们考虑具有单个空间维度x和视图维度u的"平面"光场，但请注意所有发现都适用于具有两个空间和两个视图维度的一般光场。

## 3.1 奈奎斯特速率视图采样

关于光场采样的初步工作[Chai et al. 2000]推导出，忽略遮挡和非朗伯效应的光场的傅里叶支持位于双楔形内，其边界由最小和最大场景深度z_min和z_max设定，如图2所示。Zhang和Chen [2003]表明，遮挡扩展了光场的傅里叶支持，因为遮挡物将由于更远场景内容的光场频谱与位于对应于遮挡物深度的线上的核进行卷积。考虑遮挡的光场傅里叶支持受到最近遮挡物卷积对应于最远场景内容的线的影响限制，导致如图3a所示的平行四边形形状，该形状只能以双楔形的一半密度打包。对于具有遮挡的光场，所需的最大相机采样间隔Δu为：

Δu ≤ 1 / [2K_x f (1/z_min - 1/z_max)] (1)

K_x是采样光场中表示的最高空间频率，由连续光场中的最高空间频率B_x和相机空间分辨率Δx决定：

K_x = min(B_x, 1/(2Δx)) (2)

## 3.2 MPI场景表示与渲染

MPI场景表示[Zhou et al. 2018]由一组正面平行的RGBα平面组成，在参考相机视锥体内均匀采样视差（见图4）。我们可以通过使用"over"操作符[Porter and Duff 1984]沿着射线alpha合成颜色到新视图相机中，从MPI在局部邻域内连续值相机姿态渲染新视图。这种渲染过程等效于将每个MPI平面重新投影到新视图相机的传感器平面上，并从后到前alpha合成MPI平面，正如在早期体积渲染工作中观察到的那样[Lacroute and Levoy 1994]。MPI可以被视为局部光场的编码，类似于分层光场显示器[Wetzstein et al. 2011, 2012]。

## 3.3 视图采样率减少

光场采样理论[Chai et al. 2000]还表明，将场景分解为D个深度范围并分别采样每个范围内的光场，允许相机采样间隔增加D倍。这是因为每个深度范围内场景内容发射的光场频谱位于更紧的双楔形内，可以比完整场景的双楔形频谱更紧密地打包D倍。因此，可以为每个深度范围使用具有不同剪切的更紧重建滤波器，如图2b所示。忽略遮挡效应的重建光场仅仅是所有层重建的总和，如图2c所示。

然而，将此分析扩展到处理遮挡并不简单，因为所有深度范围的傅里叶频谱的并集比具有遮挡的原始光场具有更小的支持，如图3c所示。相反，我们观察到，如果给定每个层的相应每视图不透明度或屏蔽场[Lanman et al. 2008]，则在尊重遮挡的情况下从这些深度范围光场重建完整场景光场会容易得多。然后我们可以轻松地从后到前alpha合成深度范围光场来计算完整场景光场。

每个alpha合成步骤通过将先前累积的光场频谱与遮挡深度层的频谱进行卷积来增加傅里叶支持。正如信号处理中众所周知的，两个频谱的卷积具有等于原始频谱带宽之和的傅里叶带宽。图3b说明，考虑遮挡时，每个深度范围光场的傅里叶支持平行四边形的宽度为：

2K_x f (1/z_min - 1/z_max) / D (3)

因此，完整场景的重建光场将享有完整的傅里叶支持宽度。

我们将此分析应用于我们的算法，通过将每个相机采样位置预测的MPI层解释为非重叠深度范围内场景内容的视图样本，并注意到为每个深度范围应用最优重建滤波器[Chai et al. 2000]等效于从相邻MPI重新投影然后混合预乘RGBα平面。我们的MPI层与传统光场采样中考虑的分层渲染不同，因为我们为每个层预测不透明度以及颜色，这允许我们在合成深度层光场时正确尊重遮挡。

总之，我们通过利用我们预测的不透明度扩展了分层光场采样框架以正确处理遮挡，并表明这仍然允许我们将所需相机采样间隔增加D倍：

Δu ≤ D / [2K_x f (1/z_min - 1/z_max)] (4)

我们的框架进一步与经典分层光场采样不同，因为每个MPI是在具有有限视场的参考相机视锥体内采样的，而不是先前分析[Chai et al. 2000; Zhang and Chen 2003]中假设的无限视场。为了使MPI预测过程成功，场景边界体积内的每个点应落在至少两个相邻采样视图的视锥体内。然后所需相机采样间隔Δu另外受限于：

Δu ≤ (W Δx z_min) / (2f) (5)

其中W是每个采样视图的图像宽度（像素）。整体相机采样间隔必须满足两个约束：

Δu ≤ min(D / [2K_x f (1/z_min - 1/z_max)], (W Δx z_min) / (2f)) (6)

## 3.4 视图采样的图像空间解释

将所需相机采样率解释为任何场景点在相邻输入视图之间的最大像素视差d_max是有用的。如果我们设置z_max = ∞以允许具有无限深度内容的场景，并另外设置K_x = 1/(2Δx)以允许高达最大可表示频率的空间频率：

(Δu f) / (Δx z_min) = d_max ≤ min(D, W/2) (7)

简单来说，相邻视图之间最近场景点的最大视差必须小于min(D, W/2)像素。当D = 1时，这个不等式简化为奈奎斯特边界：视图之间的最大视差为1像素。

总之，将每个视图样本提升为具有D个深度层的MPI场景表示允许我们将所需视图采样率减少D倍，直到立体几何估计所需的视场重叠。真实3D场景的光场必须在两个观察方向上采样，因此这个好处复合为D²的采样减少。第6.1节通过实验验证了我们的算法性能与这一理论分析相匹配。第7.1节描述了我们如何应用上述理论以及我们深度学习管道的经验性能来为用户规定实用采样指南。

---

# 4 实践视图合成管道

我们提出了一种实用且鲁棒的方法，用于从一组输入图像及其相机姿态合成新视图。我们的方法首先使用CNN将每个捕获的输入图像提升为MPI，然后通过混合附近MPI的渲染结果来重建新视图。图1和图5可视化了这个管道。我们在第7节讨论了我们方法实现的实用图像捕获过程。

## 4.1 局部光场扩展的MPI预测

我们管道的第一步是使用MPI场景表示将每个采样视图扩展为局部光场。我们的MPI预测管道以五个视图作为输入：要扩展的参考视图及其在3D空间中的四个最近邻居。每个图像被重新投影到D个深度平面，在参考视图视锥体内线性采样视差，形成5个平面扫描体积（PSV），大小为H × W × D × 3。

我们的3D CNN以这5个PSV作为输入，沿着通道维度连接。这个CNN为每个MPI坐标(x, y, d)输出一个不透明度α，以及一组5个颜色选择权重，这些权重在每个MPI坐标处总和为1。这些权重将输出MPI中的RGB值参数化为输入PSV的加权组合。直观地说，每个预测的MPI在每个MPI坐标处从每个输入PSV中该坐标的像素颜色"软选择"其颜色值。我们特别使用这种RGB参数化而不是Zhou等人[2018]提出的前景+背景参数化，因为他们的方法不允许MPI直接包含从参考视图遮挡但在其他输入视图中可见的内容。

此外，我们从原始版本增强了MPI预测CNN架构，使用3D卷积层而不是原始的2D卷积层，以便我们的架构在高度、宽度和深度维度上完全卷积。这使我们能够预测具有可变平面数量D的MPI，以便我们可以联合选择视图和视差采样密度以满足方程7。表2验证了能够改变MPI平面数量以正确匹配我们推导的采样要求的好处，这得益于我们使用3D卷积。我们的完整网络架构可以在附录B中找到。

## 4.2 通过混合进行连续视图重建

如第3节所述，我们将插值视图重建为来自多个附近MPI的渲染的加权组合。这有效地将我们的局部光场近似组合成一个光场，其近平面跨越捕获输入视图的范围，远平面由输入视图的视场决定。与标准光场渲染一样，这允许在由光场中的射线组成的视图范围内进行无约束3D平移和旋转的新视图路径。

我们渲染过程中的一个重要细节是，我们在混合时考虑每个MPI渲染的累积alpha值。这允许每个MPI渲染"填充"从其他相机视图遮挡的内容。

我们的MPI预测网络使用一组RGB图像Ck及其相机姿态pk来产生一组MPI Mk（每个对应一个输入图像）。要使用预测的MPI Mk渲染具有姿态pt的新视图，我们将每个RGBα MPI平面单应性变换到目标姿态pt的参考框架中，然后从后到前alpha合成变换后的平面。这产生一个RGB图像和一个alpha图像，我们分别表示为Ct,k和αt,k（下标t,k表示输出是在姿态pt处使用姿态pk处的MPI渲染的）。

由于单个MPI单独不一定包含从新相机姿态可见的所有内容（由于遮挡和视场问题），我们通过混合来自多个MPI的渲染RGB图像Ct,k来生成最终的RGB输出Ct，如图5所示。我们使用标量混合权重wt,k，每个由相应的累积alpha图像αt,k调制，并归一化以便得到的渲染图像完全不透明（α = 1）：

Ct = Σk wt,k αt,k Ct,k / Σk wt,k αt,k (8)

有关通过累积alpha值调制混合权重防止Ct中伪影的示例，请参见图6。表2表明，与使用单个MPI和不使用累积alpha混合多个MPI渲染相比，使用alpha混合在数量上给出了优越的结果。

混合权重wt,k可以是任何足够平滑的滤波器。对于在规则网格上采样的数据，我们使用来自四个最近MPI的双线性插值，而不是理想的sinc函数插值，以提高效率并由于采样视图数量有限。对于不规则采样数据，我们使用五个最近MPI，并取wt,k ∝ exp(-γ ℓ(pt, pk))。这里ℓ(pt, pk)是姿态pt和pk的平移向量之间的L2距离，常数γ定义为f/(D zmin)，给定焦距f、到场景的最小距离zmin和平面数量D。（注意，量fℓ/zmin表示ℓ转换为像素视差单位。）

我们在相邻MPI之间混合的策略对于渲染非朗伯效应特别有效。对于一般曲面，镜面反射的虚拟表观深度随视点变化[Swaminathan et al. 2002]。因此，镜面反射在光场的极线切片中显示为曲线，而漫反射点显示为直线。我们每个预测的MPI可以通过将镜面反射放置在单个虚拟深度来表示局部范围视图的镜面反射。图7说明了我们的渲染过程如何通过混合局部线性近似来有效地模拟光场中镜面反射的曲线，与单个MPI提供的有限外推形成对比。

---

# 5 训练我们的视图合成管道

## 5.1 训练数据集

我们使用自然场景的渲染和真实图像训练我们的视图合成管道。使用合成训练数据关键使我们能够轻松生成具有与测试时预期相似的输入视图和场景深度分布的大数据集，而使用真实数据帮助我们泛化到真实世界照明和反射效应以及姿态估计中的小误差。

我们的合成训练集包括从SUNCG [Song et al. 2017]和UnrealCV [Qiu et al. 2017]数据集渲染的图像。SUNCG包含45,000个简化的房屋和房间环境，具有纹理映射表面和低几何复杂性。UnrealCV仅包含几个大规模环境，但它们以极端细节建模和渲染，提供几何复杂性、纹理多样性和非朗伯反射效应。

我们的真实训练数据集包括来自我们手持手机捕获的24个场景，每个场景有20-30张图像。我们使用COLMAP运动结构[Schönberger and Frahm 2016]实现来计算我们真实图像的姿态。

## 5.2 训练过程

对于每个训练步骤，我们采样两组各5个视图作为输入，以及一个单独保留的目标视图用于监督。我们首先使用MPI预测网络预测两个MPI，每个来自一组5个输入。接下来，我们从两个MPI渲染目标新视图，并使用累积alpha值混合这些渲染，如方程8所述。

训练损失仅仅是渲染新视图的图像重建损失。我们遵循MPI预测的原始工作[Zhou et al. 2018]，并使用由Chen和Koltun [2017]实现的VGG网络激活感知损失，该损失已被一致证明优于标准图像重建损失[Huang et al. 2018; Zhang et al. 2018]。我们能够仅监督最终混合渲染，因为我们的固定渲染和混合函数都是可微分的。通过这个混合步骤学习训练我们的MPI预测网络在每个MPI的不确定区域留下alpha"洞"，期望这些内容将由另一个相邻MPI正确渲染，如图6所示。

在实践中，通过混合训练比训练单个MPI慢，因此我们首先训练网络仅从一个MPI渲染新视图进行500k次迭代，然后训练完整管道（混合来自两个不同MPI的视图）进行100k次迭代。为了微调网络处理真实数据，我们在我们的小真实数据集上额外训练10k次迭代。我们使用320 × 240分辨率和最多128个平面用于SUNCG训练数据，以及640 × 480分辨率和最多32个平面用于UnrealCV训练数据，由于GPU内存限制。我们在Tensorflow [Abadi et al. 2015]中实现我们的完整管道，并使用Adam [Kingma and Ba 2015]优化MPI预测网络参数，学习率为2 × 10⁻⁴，批大小为1。我们将训练管道拆分到两个Nvidia RTX 2080Ti GPU上，使用一个GPU生成每个MPI。

---

# 6 实验评估

我们在数量上和定性上验证了我们方法的规定采样优势以及渲染欠采样高达4000倍的光场高保真新视图的能力，并证明我们的算法在规则视图插值方面优于最先进的方法。图9展示了在具有复杂几何（Fern和T-Rex）和高度非朗伯场景（Air Plants和Pond）上的这些定性比较，这些场景大多数视图合成算法处理不好。

对于所有数量比较（表2），我们使用从未用于生成任何训练数据的UnrealCV [Qiu et al. 2017]环境渲染的合成测试集。我们的测试集包含8个场景，每个以640 × 480分辨率渲染，并在8种不同的视图采样密度下，使得相邻输入视图之间的最大视差范围从1到256像素（输入视图之间的最大视差为1像素对应于奈奎斯特速率视图采样）。我们将数量比较限制在渲染图像，因为奈奎斯特速率网格采样光场将需要至少384²个相机视图来生成类似的测试集，并且据我们所知，不存在如此密集采样的真实光场数据集。我们使用标准PSNR和SSIM指标以及最先进的LPIPS [Zhang et al. 2018]感知指标报告数量性能，该指标基于调整以匹配人类图像相似性判断的神经网络激活的加权组合。

最后，我们的伴随视频显示了超过60个额外真实世界场景的结果。这些渲染完全由脚本自动创建，该脚本仅将捕获图像集和期望的输出视图路径作为输入，突出了我们方法的实用性和鲁棒性。

## 6.1 采样理论验证

我们的方法能够渲染高质量新视图，同时显著降低与标准光场插值相比所需的输入视图采样密度。图8显示，只要我们将每个MPI中的平面数量与输入视图之间的最大像素视差匹配，我们的方法能够以高达d_max = 64像素的输入视图样本之间的视差渲染具有奈奎斯特级别感知质量的新视图。我们假设我们无法从具有最大128像素视差的输入图像匹配奈奎斯特质量是由于遮挡效应。随着相邻视图之间的最大视差增加，任何非前景场景点被较少输入视图采样的可能性增加。这增加了深度估计的难度，并需要CNN在极端情况下幻觉遮挡点的外观和深度，这些点在输入视图中都没有被采样。

图8还显示，一旦我们的采样边界满足，添加额外平面不会增加性能。例如，在32像素视差时，从8增加到16到32平面减少LPIPS误差，但从32到128平面性能保持恒定。这验证了对于高达64像素视差的场景，我们的采样边界是紧的，并且我们的方法能够实现我们理论分析预测的采样减少。

## 6.2 与最先进方法的比较

我们与最先进的视图合成方法进行比较，包括Soft3D [Penner and Zhang 2017]、DeepStereo [Flynn et al. 2016]和光场插值（LFI）。我们使用作者提供的实现，并在可能的情况下使用相同的训练数据重新训练这些方法。对于DeepStereo，我们使用作者提供的预训练模型，因为他们的训练管道需要密集采样的光场，我们无法为我们的测试集生成。

表2显示，我们的方法在所有视图采样密度下在数量上优于所有比较方法。Soft3D在低视差下表现良好，但在高视差下性能显著下降，因为其启发式几何估计在稀疏采样下变得不可靠。DeepStereo在低视差下表现良好，但在高视差下产生模糊结果，因为它为每个新视图单独预测深度，导致视图之间的不一致。光场插值在奈奎斯特采样下表现最佳，但在欠采样下产生严重的混叠伪影。

我们的方法在所有采样密度下保持高质量，因为我们的端到端训练管道学习预测局部一致的场景表示，这些表示可以混合以渲染新视图。此外，我们的方法能够渲染非朗伯效应，如图9所示，而比较方法在这些场景中产生伪影。

---

# 7 实际应用

我们开发了一个增强现实智能手机应用程序，指导用户捕获场景的输入图像，并应用我们推导的采样指南。该应用程序显示一个覆盖层，指示用户应如何移动相机以捕获所需视图采样密度的图像。具体来说，应用程序显示一个边界框，表示相机最近物体的最大允许移动，如方程7所述。

在实践中，我们建议用户将D设置为64，这对应于高达64像素的视差，并允许高达4000倍的视图采样减少。对于大多数场景，这对应于相机移动约10-20厘米，具体取决于场景深度和相机焦距。

一旦捕获了输入图像，我们使用COLMAP运动结构[Schönberger and Frahm 2016]实现计算相机姿态。然后我们使用我们的算法处理图像以生成MPI表示。最后，我们开发了移动和桌面查看器应用程序，可以实时从我们预测的MPI渲染新视图。这些查看器允许用户在捕获的场景中自由导航，提供沉浸式虚拟体验。

我们的方法特别适用于以下应用：
- **虚拟房地产游览**：允许用户虚拟探索房产，从任何角度查看房间
- **电子商务**：允许客户从不同角度查看产品
- **文化遗产保护**：数字化和虚拟访问历史遗址
- **教育和培训**：创建交互式学习环境

---

# 8 结论

我们提出了一种实用且鲁棒的视图合成方法，该方法具有规定采样指南，允许用户可靠地捕获和渲染复杂真实世界场景的新视图。我们的方法基于光场采样框架，我们扩展了该框架以处理遮挡和非朗伯效应。我们表明，通过将每个输入视图提升为局部光场表示并通过混合相邻表示来渲染新视图，我们可以在理论上将所需视图数量减少D²倍，在实践中减少高达4000倍。

我们的实验验证了我们推导的采样边界，并证明我们的方法在数量上和定性上优于最先进的视图合成方法。此外，我们开发了实用工具，包括增强现实捕获应用程序和实时查看器，使我们的方法对最终用户易于访问。

未来的工作包括扩展我们的方法以处理动态场景，改进对极端遮挡的处理，以及探索更高效的表示和渲染技术。我们相信我们的工作为实用视图合成奠定了基础，并为虚拟现实和增强现实应用开辟了新的可能性。
