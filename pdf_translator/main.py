#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PDFç¿»è¯‘å·¥å…·ä¸»ç¨‹åº
"""

import os
import sys
import argparse
import time
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pdf_parser import PDFParser
from translator import TranslationManager, TranslationError
from config import (
    TRANSLATION_CONFIG,
    DEFAULT_SOURCE_LANG,
    DEFAULT_TARGET_LANG,
    SUPPORTED_LANGUAGES
)


class PDFTranslator:
    """PDFç¿»è¯‘å™¨ä¸»ç±»"""
    
    def __init__(self, engine: str = "libre", google_api_key: str = None,
                 aliyun_access_key_id: str = None, aliyun_access_key_secret: str = None):
        """
        åˆå§‹åŒ–PDFç¿»è¯‘å™¨
        
        Args:
            engine: ç¿»è¯‘å¼•æ“ ("libre", "google" æˆ– "aliyun")
            google_api_key: Google Cloud Translation APIå¯†é’¥
            aliyun_access_key_id: é˜¿é‡Œäº‘AccessKey ID
            aliyun_access_key_secret: é˜¿é‡Œäº‘AccessKey Secret
        """
        self.pdf_parser = PDFParser()
        # å»¶è¿Ÿåˆå§‹åŒ–ç¿»è¯‘ç®¡ç†å™¨ï¼Œåªåœ¨éœ€è¦ç¿»è¯‘æ—¶åˆ›å»º
        self._engine = engine
        self._google_api_key = google_api_key
        self._aliyun_access_key_id = aliyun_access_key_id
        self._aliyun_access_key_secret = aliyun_access_key_secret
        self._translation_manager = None
        self.max_chunk_size = TRANSLATION_CONFIG["max_chunk_size"]
    
    @property
    def translation_manager(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ç¿»è¯‘ç®¡ç†å™¨"""
        if self._translation_manager is None:
            self._translation_manager = TranslationManager(
                self._engine, 
                self._google_api_key, 
                self._aliyun_access_key_id, 
                self._aliyun_access_key_secret
            )
        return self._translation_manager
    
    def extract_pdf_text(self, pdf_path: str, output_path: str = None) -> str:
        """
        ç¬¬ä¸€æ­¥ï¼šæå–PDFæ–‡æœ¬å¹¶ä¿å­˜ä¸ºä¸­é—´æ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ä¸­é—´æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ“„ ç¬¬ä¸€æ­¥ï¼šæå–PDFæ–‡æœ¬: {pdf_path}")
        
        # æå–PDFæ–‡æœ¬
        print("ğŸ“„ æ­£åœ¨è§£æPDF...")
        original_text = self.pdf_parser.extract_text_from_pdf(pdf_path)
        
        if not original_text:
            raise ValueError("PDFæ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
        
        print(f"ğŸ“ æå–åˆ° {len(original_text)} ä¸ªå­—ç¬¦çš„æ–‡æœ¬")
        
        # ç”Ÿæˆä¸­é—´æ–‡ä»¶è·¯å¾„
        if output_path is None:
            pdf_name = Path(pdf_path).stem
            output_path = f"{pdf_name}_extracted.txt"
        
        # ä¿å­˜æå–çš„æ–‡æœ¬
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# PDFæ–‡æœ¬æå–ç»“æœ\n\n")
            f.write("> æœ¬æ–‡æ¡£ä¸ºPDFæ–‡ä»¶çš„åŸå§‹æ–‡æœ¬æå–ç»“æœï¼Œä¿æŒäº†åŸæ–‡çš„ç»“æ„æ ¼å¼\n\n")
            f.write(original_text)
        
        print(f"âœ… æ–‡æœ¬æå–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def translate_extracted_text(self, extracted_file: str, output_path: str = None,
                               source_lang: str = DEFAULT_SOURCE_LANG,
                               target_lang: str = DEFAULT_TARGET_LANG) -> str:
        """
        ç¬¬äºŒæ­¥ï¼šç¿»è¯‘å·²æå–çš„æ–‡æœ¬æ–‡ä»¶
        
        Args:
            extracted_file: å·²æå–çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            ç¿»è¯‘ç»“æœä¿å­˜è·¯å¾„
        """
        print(f"ğŸŒ ç¬¬äºŒæ­¥ï¼šç¿»è¯‘æå–çš„æ–‡æœ¬: {extracted_file}")
        
        # è¯»å–æå–çš„æ–‡æœ¬
        with open(extracted_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è·³è¿‡æ–‡ä»¶å¤´ä¿¡æ¯ï¼Œåªæå–å®é™…æ–‡æœ¬å†…å®¹
        lines = content.split('\n')
        actual_text_lines = []
        in_content = False
        
        for line in lines:
            if line.startswith('# PDFæ–‡æœ¬æå–ç»“æœ') or line.startswith('> æœ¬æ–‡æ¡£ä¸ºPDFæ–‡ä»¶çš„åŸå§‹æ–‡æœ¬æå–ç»“æœ'):
                continue
            if line.strip() == '':
                continue
            actual_text_lines.append(line)
        
        original_text = '\n'.join(actual_text_lines)
        
        if not original_text.strip():
            raise ValueError("æå–çš„æ–‡æœ¬æ–‡ä»¶ä¸ºç©º")
        
        print(f"ğŸ“ è¯»å–åˆ° {len(original_text)} ä¸ªå­—ç¬¦çš„æ–‡æœ¬")
        
        # åˆ†å‰²æ–‡æœ¬ä¸ºé€‚åˆç¿»è¯‘çš„å—
        text_chunks = self.pdf_parser.split_text_into_chunks(original_text, self.max_chunk_size)
        
        # ç¿»è¯‘æ¯ä¸ªæ–‡æœ¬å—
        print("ğŸŒ å¼€å§‹ç¿»è¯‘...")
        translated_chunks = []
        
        with tqdm(total=len(text_chunks), desc="ç¿»è¯‘è¿›åº¦", unit="å—") as pbar:
            for i, chunk in enumerate(text_chunks):
                try:
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    if i > 0:
                        time.sleep(1)  # 1ç§’å»¶è¿Ÿ
                    
                    translated_chunk = self.translation_manager.translate_text(
                        chunk, source_lang, target_lang
                    )
                    translated_chunks.append(translated_chunk)
                    pbar.update(1)
                    
                except TranslationError as e:
                    print(f"\nâŒ ç¿»è¯‘å— {i+1} å¤±è´¥: {e}")
                    # ä¿å­˜å·²ç¿»è¯‘çš„éƒ¨åˆ†
                    translated_chunks.append(f"[ç¿»è¯‘å¤±è´¥] {chunk}")
                    pbar.update(1)
                    continue
        
        # åˆå¹¶ç¿»è¯‘ç»“æœï¼Œæ™ºèƒ½å¤„ç†æ¢è¡Œ
        translated_text = self._merge_translated_chunks(translated_chunks)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_path is None:
            file_name = Path(extracted_file).stem.replace('_extracted', '')
            output_path = f"{file_name}_translated.txt"
        
        # ä¿å­˜ç¿»è¯‘ç»“æœ
        self._save_translation_result(original_text, translated_text, output_path)
        
        print(f"âœ… ç¿»è¯‘å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def translate_pdf(self, pdf_path: str, output_path: str = None, 
                      source_lang: str = DEFAULT_SOURCE_LANG, 
                      target_lang: str = DEFAULT_TARGET_LANG) -> str:
        """
        ç¿»è¯‘å•ä¸ªPDFæ–‡ä»¶ï¼ˆä¸€æ­¥å®Œæˆï¼‰
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            
        Returns:
            ç¿»è¯‘ç»“æœä¿å­˜è·¯å¾„
        """
        print(f"ğŸš€ å¼€å§‹ç¿»è¯‘PDF: {pdf_path}")
        
        # æå–PDFæ–‡æœ¬
        print("ğŸ“„ æ­£åœ¨è§£æPDF...")
        original_text = self.pdf_parser.extract_text_from_pdf(pdf_path)
        
        if not original_text:
            raise ValueError("PDFæ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
        
        print(f"ğŸ“ æå–åˆ° {len(original_text)} ä¸ªå­—ç¬¦çš„æ–‡æœ¬")
        
        # åˆ†å‰²æ–‡æœ¬ä¸ºé€‚åˆç¿»è¯‘çš„å—
        text_chunks = self.pdf_parser.split_text_into_chunks(original_text, self.max_chunk_size)
        
        # ç¿»è¯‘æ¯ä¸ªæ–‡æœ¬å—
        print("ğŸŒ å¼€å§‹ç¿»è¯‘...")
        translated_chunks = []
        
        with tqdm(total=len(text_chunks), desc="ç¿»è¯‘è¿›åº¦", unit="å—") as pbar:
            for i, chunk in enumerate(text_chunks):
                try:
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    if i > 0:
                        time.sleep(1)  # 1ç§’å»¶è¿Ÿ
                    
                    translated_chunk = self.translation_manager.translate_text(
                        chunk, source_lang, target_lang
                    )
                    translated_chunks.append(translated_chunk)
                    pbar.update(1)
                    
                except TranslationError as e:
                    print(f"\nâŒ ç¿»è¯‘å— {i+1} å¤±è´¥: {e}")
                    # ä¿å­˜å·²ç¿»è¯‘çš„éƒ¨åˆ†
                    translated_chunks.append(f"[ç¿»è¯‘å¤±è´¥] {chunk}")
                    pbar.update(1)
                    continue
        
        # åˆå¹¶ç¿»è¯‘ç»“æœï¼Œæ™ºèƒ½å¤„ç†æ¢è¡Œ
        translated_text = self._merge_translated_chunks(translated_chunks)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_path is None:
            pdf_name = Path(pdf_path).stem
            output_path = f"{pdf_name}_translated.txt"
        
        # ä¿å­˜ç»“æœï¼ˆåŸæ–‡+è¯‘æ–‡å¯¹ç…§ï¼‰
        self._save_translation_result(original_text, translated_text, output_path)
        
        print(f"âœ… ç¿»è¯‘å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def _save_translation_result(self, original_text: str, translated_text: str, output_path: str):
        """
        ä¿å­˜ç¿»è¯‘ç»“æœï¼ˆä»…ä¿å­˜è¯‘æ–‡ï¼ŒMarkdownæ ¼å¼ï¼‰
        
        Args:
            original_text: åŸæ–‡
            translated_text: è¯‘æ–‡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# PDFç¿»è¯‘ç»“æœ\n\n")
            f.write("> æœ¬æ–‡æ¡£ä¸ºPDFæ–‡ä»¶çš„ç¿»è¯‘ç»“æœï¼Œä¿æŒäº†åŸæ–‡çš„ç»“æ„æ ¼å¼\n\n")
            f.write(translated_text)
    
    def batch_translate(self, input_dir: str, output_dir: str = None,
                       source_lang: str = DEFAULT_SOURCE_LANG,
                       target_lang: str = DEFAULT_TARGET_LANG):
        """
        æ‰¹é‡ç¿»è¯‘ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
        """
        input_path = Path(input_dir)
        
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = list(input_path.glob("*.pdf"))
        
        if not pdf_files:
            print(f"âŒ åœ¨ç›®å½• {input_dir} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            output_dir = input_path / "translations"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰¹é‡ç¿»è¯‘
        success_count = 0
        for pdf_file in pdf_files:
            try:
                output_file = output_dir / f"{pdf_file.stem}_translated.txt"
                self.translate_pdf(
                    str(pdf_file),
                    str(output_file),
                    source_lang,
                    target_lang
                )
                success_count += 1
                
                # æ–‡ä»¶é—´æ·»åŠ è¾ƒé•¿å»¶è¿Ÿ
                time.sleep(3)
                
            except Exception as e:
                print(f"âŒ ç¿»è¯‘æ–‡ä»¶ {pdf_file.name} å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ‰ æ‰¹é‡ç¿»è¯‘å®Œæˆï¼æˆåŠŸç¿»è¯‘ {success_count}/{len(pdf_files)} ä¸ªæ–‡ä»¶")
    
    def _merge_translated_chunks(self, chunks: list) -> str:
        """
        æ™ºèƒ½åˆå¹¶ç¿»è¯‘å—ï¼Œä¼˜åŒ–æ¢è¡Œæ’ç‰ˆ
        
        Args:
            chunks: ç¿»è¯‘å—åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ–‡æœ¬
        """
        if not chunks:
            return ""
        
        merged_text = ""
        
        for i, chunk in enumerate(chunks):
            # æ¸…ç†å½“å‰å—çš„æ¢è¡Œ
            cleaned_chunk = chunk.strip()
            
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå—ï¼Œç›´æ¥æ·»åŠ 
            if i == 0:
                merged_text = cleaned_chunk
                continue
            
            # æ£€æŸ¥å‰ä¸€ä¸ªå—çš„ç»“å°¾å’Œå½“å‰å—çš„å¼€å¤´
            prev_chunk_ends_with_punctuation = merged_text and merged_text[-1] in 'ã€‚ï¼ï¼Ÿ.!?'
            current_chunk_starts_with_space = cleaned_chunk and cleaned_chunk[0].isspace()
            
            # æ™ºèƒ½æ·»åŠ åˆ†éš”ç¬¦
            if prev_chunk_ends_with_punctuation:
                # å¦‚æœå‰ä¸€ä¸ªå—ä»¥æ ‡ç‚¹ç»“å°¾ï¼Œæ·»åŠ ä¸¤ä¸ªæ¢è¡Œï¼ˆæ–°æ®µè½ï¼‰
                merged_text += "\n\n" + cleaned_chunk
            elif current_chunk_starts_with_space:
                # å¦‚æœå½“å‰å—ä»¥ç©ºæ ¼å¼€å¤´ï¼Œç›´æ¥è¿æ¥
                merged_text += cleaned_chunk
            else:
                # å…¶ä»–æƒ…å†µæ·»åŠ ä¸€ä¸ªæ¢è¡Œ
                merged_text += "\n" + cleaned_chunk
        
        return merged_text


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFç¿»è¯‘å·¥å…· - å°†PDFæ–‡ä»¶ç¿»è¯‘ä¸ºæŒ‡å®šè¯­è¨€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä¸€æ­¥ç¿»è¯‘å•ä¸ªPDFæ–‡ä»¶
  python main.py --input paper.pdf --output translated.txt
  
  # ä¸¤æ­¥ç¿»è¯‘ï¼šç¬¬ä¸€æ­¥æå–æ–‡æœ¬
  python main.py --step extract --input paper.pdf --output extracted.txt
  
  # ä¸¤æ­¥ç¿»è¯‘ï¼šç¬¬äºŒæ­¥ç¿»è¯‘æå–çš„æ–‡æœ¬
  python main.py --step translate --input extracted.txt --output translated.txt
  
  # æ‰¹é‡ç¿»è¯‘ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶
  python main.py --input papers/ --output translations/
  
  # ä½¿ç”¨é˜¿é‡Œäº‘æœºå™¨ç¿»è¯‘ï¼ˆéœ€è¦AccessKeyï¼‰
  python main.py --input paper.pdf --engine aliyun --aliyun-access-key-id YOUR_ID --aliyun-access-key-secret YOUR_SECRET
  
  # æŒ‡å®šç¿»è¯‘è¯­è¨€
  python main.py --input paper.pdf --source en --target zh
        """
    )
    
    parser.add_argument("--step", choices=["extract", "translate", "auto"], default="auto",
                       help="ç¿»è¯‘æ­¥éª¤: extract(ä»…æå–æ–‡æœ¬), translate(ä»…ç¿»è¯‘æ–‡æœ¬), auto(è‡ªåŠ¨å®Œæˆ)")
    parser.add_argument("--input", "-i", required=True, 
                       help="è¾“å…¥PDFæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", 
                       help="è¾“å‡ºæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--engine", "-e", choices=["libre", "google", "aliyun"], 
                       default="libre", help="ç¿»è¯‘å¼•æ“ (é»˜è®¤: libre)")
    parser.add_argument("--api-key", 
                       help="Google Cloud Translation APIå¯†é’¥")
    parser.add_argument("--aliyun-access-key-id", 
                       help="é˜¿é‡Œäº‘AccessKey ID")
    parser.add_argument("--aliyun-access-key-secret", 
                       help="é˜¿é‡Œäº‘AccessKey Secret")
    parser.add_argument("--source", "-s", default=DEFAULT_SOURCE_LANG,
                       help=f"æºè¯­è¨€ä»£ç  (é»˜è®¤: {DEFAULT_SOURCE_LANG})")
    parser.add_argument("--target", "-t", default=DEFAULT_TARGET_LANG,
                       help=f"ç›®æ ‡è¯­è¨€ä»£ç  (é»˜è®¤: {DEFAULT_TARGET_LANG})")
    
    args = parser.parse_args()
    
    # éªŒè¯è¯­è¨€ä»£ç 
    if args.source not in SUPPORTED_LANGUAGES:
        print(f"âŒ ä¸æ”¯æŒçš„æºè¯­è¨€ä»£ç : {args.source}")
        print(f"æ”¯æŒçš„è¯­è¨€: {', '.join(SUPPORTED_LANGUAGES.keys())}")
        return 1
    
    if args.target not in SUPPORTED_LANGUAGES:
        print(f"âŒ ä¸æ”¯æŒçš„ç›®æ ‡è¯­è¨€ä»£ç : {args.target}")
        print(f"æ”¯æŒçš„è¯­è¨€: {', '.join(SUPPORTED_LANGUAGES.keys())}")
        return 1
    
    try:
        # åªæœ‰åœ¨éœ€è¦ç¿»è¯‘æ—¶æ‰æ£€æŸ¥é˜¿é‡Œäº‘å¯†é’¥
        if args.step != "extract":
            if not args.aliyun_access_key_id or not args.aliyun_access_key_secret:
                args.aliyun_access_key_id = os.getenv('ALIYUN_ACCESS_KEY_ID')
                args.aliyun_access_key_secret = os.getenv('ALIYUN_ACCESS_KEY_SECRET')
            if not args.aliyun_access_key_id or not args.aliyun_access_key_secret:
                print("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡: ALIYUN_ACCESS_KEY_ID å’Œ ALIYUN_ACCESS_KEY_SECRET")
                print("ä½¿ç”¨æ–¹æ³•:")
                print("  export ALIYUN_ACCESS_KEY_ID=your_access_key_id")
                print("  export ALIYUN_ACCESS_KEY_SECRET=your_access_key_secret")
                print("  python test_aliyun.py")
                return
        
        translator = PDFTranslator(
            args.engine, 
            args.api_key,
            args.aliyun_access_key_id,
            args.aliyun_access_key_secret
        )
        
        if args.step == "extract":
            # ä»…æå–æ–‡æœ¬
            if not os.path.isfile(args.input):
                print(f"âŒ è¾“å…¥è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {args.input}")
                return 1
            
            if not args.input.lower().endswith('.pdf'):
                print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸æ˜¯PDFæ ¼å¼: {args.input}")
                return 1
            
            translator.extract_pdf_text(args.input, args.output)
            
        elif args.step == "translate":
            # ä»…ç¿»è¯‘æ–‡æœ¬
            if not os.path.isfile(args.input):
                print(f"âŒ è¾“å…¥è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {args.input}")
                return 1
            
            translator.translate_extracted_text(
                args.input,
                args.output,
                args.source,
                args.target
            )
            
        else:  # auto mode
            if os.path.isfile(args.input):
                # å•ä¸ªæ–‡ä»¶ç¿»è¯‘
                translator.translate_pdf(
                    args.input, 
                    args.output,
                    args.source,
                    args.target
                )
            elif os.path.isdir(args.input):
                # æ‰¹é‡ç¿»è¯‘
                translator.batch_translate(
                    args.input,
                    args.output,
                    args.source,
                    args.target
                )
            else:
                print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input}")
                return 1
            
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
