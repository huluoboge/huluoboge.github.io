#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
PDFè§£ææ¨¡å—
ç”¨äºä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹
"""

import pdfplumber
import os
import re
from typing import List, Tuple, Optional


class PDFParser:
    def __init__(self):
        self.supported_formats = ['.pdf']
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        ä»PDFæ–‡ä»¶ä¸­æå–æ‰€æœ‰æ–‡æœ¬
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
        
        if not pdf_path.lower().endswith('.pdf'):
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {pdf_path}")
        
        full_text = ""
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                print(f"ğŸ“„ æ­£åœ¨è§£æPDF: {pdf_path} (å…±{len(pdf.pages)}é¡µ)")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
                        cleaned_text = self._clean_text(page_text)
                        full_text += f"\n--- ç¬¬{page_num}é¡µ ---\n{cleaned_text}\n"
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if page_num % 10 == 0 or page_num == len(pdf.pages):
                        print(f"  å·²è§£æ {page_num}/{len(pdf.pages)} é¡µ")
        
        except Exception as e:
            raise Exception(f"è§£æPDFæ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return full_text.strip()
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æå–çš„æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)
        
        # ç§»é™¤é¡µçœ‰é¡µè„šç­‰å¸¸è§å™ªéŸ³
        text = self._remove_header_footer(text)
        
        return text.strip()
    
    def _remove_header_footer(self, text: str) -> str:
        """
        ç§»é™¤å¯èƒ½çš„é¡µçœ‰é¡µè„šå†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # è·³è¿‡å¯èƒ½çš„é¡µç ã€æ—¥æœŸç­‰é¡µçœ‰é¡µè„šå†…å®¹
            if (re.match(r'^\d+$', line) or  # çº¯æ•°å­—ï¼ˆé¡µç ï¼‰
                re.match(r'^[A-Z\s]+$', line) or  # å…¨å¤§å†™ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
                len(line) < 3):  # å¤ªçŸ­çš„æ–‡æœ¬
                continue
            
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def split_text_into_chunks(self, text: str, max_chunk_size: int = 2000) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆé€‚åˆç¿»è¯‘çš„å°å—
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        if len(text) <= max_chunk_size:
            return [text]
        
        chunks = []
        paragraphs = text.split('\n\n')
        current_chunk = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œåˆ™æ·»åŠ 
            if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:
                if current_chunk:
                    current_chunk += '\n\n' + paragraph
                else:
                    current_chunk = paragraph
            else:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                if current_chunk:
                    chunks.append(current_chunk)
                
                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
                if len(paragraph) > max_chunk_size:
                    # æŒ‰å¥å­åˆ†å‰²
                    sentences = re.split(r'[.!?]+', paragraph)
                    temp_chunk = ""
                    
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if not sentence:
                            continue
                        
                        if len(temp_chunk) + len(sentence) + 2 <= max_chunk_size:
                            if temp_chunk:
                                temp_chunk += '. ' + sentence
                            else:
                                temp_chunk = sentence
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk)
                            temp_chunk = sentence
                    
                    if temp_chunk:
                        current_chunk = temp_chunk
                else:
                    current_chunk = paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk)
        
        print(f"ğŸ“ æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(chunks)} ä¸ªç¿»è¯‘å—")
        return chunks


def main():
    """æµ‹è¯•å‡½æ•°"""
    parser = PDFParser()
    
    # æµ‹è¯•æ–‡æœ¬åˆ†å‰²
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ®µè½ã€‚" * 100
    chunks = parser.split_text_into_chunks(test_text, 100)
    print(f"æµ‹è¯•åˆ†å‰²ç»“æœ: {len(chunks)} ä¸ªå—")
    for i, chunk in enumerate(chunks[:3]):
        print(f"å— {i+1}: {chunk[:50]}...")


if __name__ == "__main__":
    main()
